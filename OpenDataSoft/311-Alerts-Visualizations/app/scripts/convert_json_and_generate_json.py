import requests, json, os, shutil, sys, datetime, decimal, functools

def remove_outside_whitespace_from_string(input_str):
    return (' ').join(input_str.split())

# returns the number of hours between two (int) Unix timestamps
def hours_between_timestamps(start, end):
    delta = end - start
    hours = float(delta)/3600
    # print(hours)
    return hours

# converts a unix timestamp to a human readable string in the local timezone
def timestamp_to_date_str(timestamp):
    lt = datetime.datetime.fromtimestamp(timestamp)
    return_str = str(lt.month) + '/' + str(lt.day) + '/' + str(lt.year)
    return return_str

'''
for every key in totals, store totals[key] / counts[key] in
averages[key]
'''
def calculate_averages(averages, counts, totals):
    for key in totals:
        val = totals[key] 

        # avg = float(val)/float(counts[key])
        # # use Decimal to round exact halves up (python rounds down by default)
        # dec = decimal.Decimal(avg)
        # rounded = dec.quantize(decimal.Decimal(str(0.001)), rounding=decimal.ROUND_UP)
        # averages[key] = float(rounded)

        # avg = decimal.Decimal(str(val))/decimal.Decimal(str(counts[key]))
        # # use Decimal to round exact halves up (python rounds down by default)
        # rounded = avg.quantize(decimal.Decimal(str(0.001)), rounding=decimal.ROUND_UP)
        # averages[key] = float(rounded)

        avg = round(float(val)/float(counts[key]), 3)
        averages[key] = avg

# return the sorted hash 'input_hash'
def sort_dict(input_hash):
    sorted_hash = {}
    for key in sorted(input_hash):
        # convert the key to a string first so that it matches the php result
        sorted_hash[str(key)] = input_hash[key]
    return sorted_hash

# # return the sorted hash 'input_hash', heys must be ints or strings
def sort_dict_ints_and_str_keys(input_hash):
    sorted_hash = {}
    
    # find out which keys are strings and which are numbers
    num_keys = [] # keys that are numbers
    str_keys = [] # keys that are strings
    for key in input_hash:
        if isinstance(key, str):
            str_keys.append(key)
        elif isinstance(key, int):
            num_keys.append(key)

    # put in the number keys first in sorted order
    for key in sorted(num_keys):
        sorted_hash[str(key)] = input_hash[key]

    # put in the string keys next in sorted order
    for key in sorted(str_keys):
        sorted_hash[key] = input_hash[key]

    return sorted_hash

def date_compare(a, b):
    a = a['date_start']
    b = b['date_start']
    delta = datetime.datetime.strptime(a, '%m/%d/%Y') - datetime.datetime.strptime(b, '%m/%d/%Y')
    return delta.total_seconds()

def construct_dataset(datasets, keys, field, hash):
    prev_date = '';

    for key in keys:
        datasets[key] = []

    hash = sorted(hash, key=functools.cmp_to_key(date_compare))

    for record in hash:
        date_start = record['date_start']
        project_type = record[field]

        if date_start == prev_date:
            for key in datasets:
                if key == project_type:
                    datasets[key][-1] += 1
        else:
            for key in datasets:
                prev_date = date_start
                if key == 'dates':
                    datasets[key].append(date_start)
                elif key == project_type:
                    if len(datasets[key]) == 0:
                        datasets[key].append(1)
                    else:
                        lastVal = datasets[key][-1]
                        datasets[key].append(lastVal + 1)
                else:
                    if len(datasets[key]) == 0:
                        datasets[key].append(0)
                    else:
                        lastVal = datasets[key][-1]
                        datasets[key].append(lastVal)


# workspace = r'\\vgisdev\apps\visualizations\QAlerts\json'
workspace = r'C:\inetpub\wwwroot\apps\visualizations\QAlerts\json'

# this is the location of the json data that is generated by the script that pulls
# QAlert data using the QScend API.
# json_path = r"M:\GIS_Projects\Qscend\request_data\response_with_date_closed.json"
json_path = r"\\vgisdata\GIS Data\GIS_Projects\Qscend\request_data\response_with_date_closed.json"

json_data = None
with open(json_path) as json_file:
    json_data = json.load(json_file)['request']

if not os.path.exists(workspace):
    os.makedirs(workspace)

department_freq = {}
district_freq = {}
typename_freq = {}
origin_freq = {}

# average requests per department
department_hours = {}
num_department_records_with_hours = {}
avg_department_hours = {}

# average requests per district
district_hours = {}
num_district_records_with_hours = {}
avg_district_hours = {}

# average requests per typename
typename_hours = {}
num_typename_records_with_hours = {}
avg_typename_hours = {}

# average requests per origin
origin_hours = {}
num_origin_records_with_hours = {}
avg_origin_hours = {}

# for department timeline
department_hash = []
department_types = []
department_datasets = {
    'dates': []
}

# for district timeline
district_hash = []
district_types = []
district_datasets = {
    'dates': []
}

# for typename timeline
typename_hash = []
typename_types = []
typename_datasets = {
    'dates': []
}

# for origin timeline
origin_hash = []
origin_types = []
origin_datasets = {
    'dates': []
}

# request code counts
open_requests = 0
closed_requests = 0
in_progress_requests = 0
on_hold_requests = 0

# for record in json_data[:10]:
for record in json_data:
    try:
        department = record['dept']
    except:
        department = None
    try:
        district = record['district']
    except:
        district = None
    try:
        typename = remove_outside_whitespace_from_string(record['typeName'])
    except:
        typename = None
    try:
        ts_closed = int(record['dateClosedUnix'])
    except:
        ts_closed = None
    try:
        ts_added = int(record['addDateUnix'])
    except:
        ts_added = None
    try:
        record_id = record['id']
    except:
        record_id = None
    hours_between = None
    try:
        request_status = record['status']
    except:
        request_status = None
    try:
        origin = record['origin']
    except:
        origin = None

    if request_status == 0:
        open_requests += 1
    elif request_status == 1:
        closed_requests += 1
    elif request_status == 3:
        in_progress_requests += 1
    elif request_status == 4:
        on_hold_requests += 1
    else:
        print('Unknown Request Type.')

    if ts_closed and ts_added:
        hours_between = hours_between_timestamps(ts_added, ts_closed)

    if department:
        if not department in department_freq:
            department_freq[department] = 0
        department_freq[department] += 1

    if district:
        if not district in district_freq:
            district_freq[district] = 0
        district_freq[district] += 1

    if typename:
        if not typename in typename_freq:
            typename_freq[typename] = 0
        typename_freq[typename] += 1

    if origin:
        if not origin in origin_freq:
            origin_freq[origin] = 0
        origin_freq[origin] += 1

    # get information for department timeline
    if ts_added and department:
        department_hash.append({
            'id' : record_id,
            'date_start' : timestamp_to_date_str(ts_added),
            'department' : department
        })
        if not department in department_types:
            department_types.append(department)

    # get information for district timeline
    if ts_added and district:
        district_hash.append({
            'id' : record_id,
            'date_start' : timestamp_to_date_str(ts_added),
            'district' : district
        })
        if not district in district_types:
            district_types.append(district)

    # get information for typename timeline
    if ts_added and typename:
        typename_hash.append({
            'id' : record_id,
            'date_start' : timestamp_to_date_str(ts_added),
            'typename' : typename
        })
        if not typename in typename_types:
            typename_types.append(typename)

    # get information for origin timeline
    if ts_added and origin:
        origin_hash.append({
            'id' : record_id,
            'date_start' : timestamp_to_date_str(ts_added),
            'origin' : origin
        })
        if not origin in origin_types:
            origin_types.append(origin)

    if department and ts_closed and ts_added:
        if not department in department_hours:
            department_hours[department] = 0
        if not department in num_department_records_with_hours:
            num_department_records_with_hours[department] = 0
        department_hours[department] += hours_between
        num_department_records_with_hours[department] += 1

    if district and ts_closed and ts_added:
        if not district in district_hours:
            district_hours[district] = 0
        if not district in num_district_records_with_hours:
            num_district_records_with_hours[district] = 0
        district_hours[district] += hours_between
        num_district_records_with_hours[district] += 1

    if typename and ts_closed and ts_added:
        if not typename in typename_hours:
            typename_hours[typename] = 0
        if not typename in num_typename_records_with_hours:
            num_typename_records_with_hours[typename] = 0
        typename_hours[typename] += hours_between
        num_typename_records_with_hours[typename] += 1

    if origin and ts_closed and ts_added:
        if not origin in origin_hours:
            origin_hours[origin] = 0
        if not origin in num_origin_records_with_hours:
            num_origin_records_with_hours[origin] = 0
        origin_hours[origin] += hours_between
        num_origin_records_with_hours[origin] += 1

# calculate averages
calculate_averages(avg_department_hours, num_department_records_with_hours, department_hours)
calculate_averages(avg_district_hours, num_district_records_with_hours, district_hours)
calculate_averages(avg_typename_hours, num_typename_records_with_hours, typename_hours)
calculate_averages(avg_origin_hours, num_origin_records_with_hours, origin_hours)

construct_dataset(department_datasets, department_types, 'department', department_hash)
construct_dataset(district_datasets, district_types, 'district', district_hash)
construct_dataset(typename_datasets, typename_types, 'typename', typename_hash)
construct_dataset(origin_datasets, origin_types, 'origin', origin_hash)

# sort hash tables
department_freq = sort_dict(department_freq)
department_hours = sort_dict(department_hours)
avg_department_hours = sort_dict(avg_department_hours)
department_datasets = sort_dict(department_datasets)
district_freq = sort_dict(district_freq)
district_hours = sort_dict(district_hours)
avg_district_hours = sort_dict(avg_district_hours)
district_datasets = sort_dict_ints_and_str_keys(district_datasets)
typename_freq = sort_dict(typename_freq)
typename_hours = sort_dict(typename_hours)
avg_typename_hours = sort_dict(avg_typename_hours)
typename_datasets = sort_dict(typename_datasets)
origin_freq = sort_dict(origin_freq)
origin_hours = sort_dict(origin_hours)
avg_origin_hours = sort_dict(avg_origin_hours)
origin_datasets = sort_dict(origin_datasets)

request_status_freq = {
    'Open' : open_requests,
    'Closed' : closed_requests,
    'In-Progress' : in_progress_requests,
    'On-Hold' : on_hold_requests
}

json_result = {
    'requestStatusFreq' : request_status_freq,
    'departmentFreq' : department_freq,
    'departmentHours' : department_hours,
    'avgDepartmentHours' : avg_department_hours,
    'departmentDatasets' : department_datasets,
    'districtFreq' : district_freq,
    'districtHours' : district_hours,
    'avgDistrictHours' : avg_district_hours,
    'districtDatasets' : district_datasets,
    'typenameFreq' : typename_freq,
    'typenameHours' : typename_hours,
    'avgTypenameHours' : avg_typename_hours,
    'typenameDatasets' : typename_datasets,
    'originFreq' : origin_freq,
    'originHours' : origin_hours,
    'avgOriginHours' : avg_origin_hours,
    'originDatasets' : origin_datasets,
    'numRecords' : len(json_data),
    'openRequests' : open_requests,
    'closedRequests' : closed_requests,
    'inProgressRequests' : in_progress_requests,
    'onHoldRequests' : on_hold_requests
}

with open(workspace + r'\visualization_data_cached.json', 'w') as outfile:
    json.dump(json_result, outfile)

# DEBUGGING

# for key in json_result:
#     debug_filename = workspace + r'\2_' + key + r'.json'
#     with open(debug_filename, 'w') as debugfile:
#         json.dump(json_result[key], debugfile)

# with open(r'\\vgisdev\apps\visualizations\QAlerts\json\visualization_data_cached_4.json') as old_result:
#     old_json = json.load(old_result)

#     print('\nMatching datasets:\n')
#     for key in json_result:
#         if json_result[key] != old_json[key]:
#             continue
#         else:
#             print(key)

#     print('\nNon-matching datasets:\n')

#     for key in json_result:
#         if json_result[key] != old_json[key]:
#             print(key)